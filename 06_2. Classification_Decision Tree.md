### Decision Tree

- **Decision Tree**
    - 일련의 분류 규칙을 통해 데이터를 분류, 회귀하는 지도 학습 모델
    - 분류, 회귀 작업에 사용되며, 복잡한 데이터 세트도 학습이 가능
    - Naive-Bayes와 같은 범주형 데이터에도 적합
    - 강력한 머신러닝 알고리즘 가운데 하나인 랜덤 포레스트의 기본 구성 요소
    - ID3, C4.5, CART, CHAID와 같은 알고리즘이 있다.
<br><br>
- **Decision Tree 생성**
    - 데이터를 분할하기 위해 가장 중요한 특징이 무엇인지 선택하는
    로컬 최적화 방법을 계속 적용하는 탐욕적 방식으로 트리를 생성
    - 학습 샘플을 Subset으로 분할하면서 생성되며, 분할의 과정은 각 Subset에 대해 
    재귀 형태로 진행
    - 각 노드에서의 분할은 특징 값을 기반으로 조건 검사를 통해 진행
    (분할 기준을 선택하기위하여 **불순도(Impurity)** 개념 사용
    - 각 속성에 대해 **Information Gain**(부모 노드와 자식 노드의 불순도 차이) 계산 후 **Information Gain**이 최대가 되는 분기 조건을 찾아서 분할
    - Subset이 동일한 클래스 레이블을 가지는 경우 또는 분할을 통한 클래스 분류가 더 이상
    의미가 없을 경우 트리 분할 작업 종료 (모든 리프 노드의 불순도가 0이 될 때까지 분할)
<br><br>
- **Impurity (불순도)**
    
    복잡성을 의미하며, 해당 범주 안에 서로 다른 데이터가 얼마나 섞여있는지를 의미한다.
    다양한 개체들이 있을수록 불순도가 높아진다.
    
    분기 기준 설정 시 현재 노드의 불순도에 비해 자식 노드의 불순도가 감소되도록 설정
    
    - **지니 계수 (Gini)**
        
        $$I(A)=1-\sum_{k=1}^mp_k^2$$
        
        지니계수의 최댓값은 0.5
        
    - **엔트로피 지수 (Entropy)**
        
        $$E=-\sum_{i=1}^kp_i log_2(p_i)$$
        

- **CART 알고리즘**
    - 불순도를 **지니 계수(Gini Index)**로 계산하는 의사결정 트리 알고리즘
    - 노드를 왼쪽, 오른쪽 자식 노드로 분할 확장하면서 트리를 생성
    - 분할 단계에서 가장 중요한 특징과 해당 값의 모든 가능한 조합을 측정 함수를 이용해 
    탐욕적으로 탐색
    - 범주형 특징의 경우 해당 특징 값을 가진 샘플들을 오른쪽 자식 노드에 할당
    - 수치형 특징의 경우 해당 값보다 큰 값을 가진 샘플들을 오른쪽 자식 노드에 할당 <br><br>
    
    **Iris 데이터를 이용하여 Decision Tree 학습 및 시각화**
    
    ```python
    # Decision Tree 학습
    from sklearn.datasets import load_iris
    iris = load_iris()
    
    X = iris.data
    y = iris.target
    
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
    
    sc = StandardScaler()
    sc.fit(X_train)
    
    X_train_std = sc.transform(X_train)
    X_test_std = sc.transform(X_test)
    
    from sklearn.tree import DecisionTreeClassifier
    iris_tree = DecisionTreeClassifier(max_depth=5, random_state=0)
    iris_tree.fit(X_train, y_train)
    
    from sklearn.metrics import accuracy_score
    y_pred_tr = iris_tree.predict(X_test)
    print('Accuracy: %.2f' % accuracy_score(y_test, y_pred_tr))
    ```
    
    ```python
    Accuracy: 0.98
    ```
    
    ```python
    # Decision Tree 시각화
    sklearn.tree import export_graphviz
    export_graphviz(iris_tree, out_file='iris.dot', feature_names=iris.feature_names, class_names=iris.target_names, rounded=True, filled=True, impurity=True)
    
    import pydot
    graph = pydot.graph_from_dot_file('iris.dot')[0]
    iris_png = graph.create_png()
    
    from IPython.core.display import Image
    Image(iris_png)
    ```
    
    ![6-2-1](https://github.com/eeeeeddy/Machine_Learning/assets/71869717/9683de9a-2b0b-494c-9fc6-e01bc245ada8)

    
- **분류 모델 성능 측정**
    
    ![6-2-2](https://github.com/eeeeeddy/Machine_Learning/assets/71869717/ab8b67c6-6203-4c97-b710-63703fb58e47)

    
    - **정확도(Accuracy)**
        
        전체 샘플에서 정확하게 예측한 샘플 수의 비율
        
        $$Accuracy =  \frac{TP+TN}{TP+TN+FP+FN}$$
        
    - **정밀도(Precision)**
        
        Positive 클래스로 예측한 샘플에서 실제 Positive 클래스에 속하는 샘플 수의 비율
        
        $$Precision = \frac{TP}{TP+FP}$$
        
    - **재현율(Recall)**
        
        실제 Positive 클래스에 속한 샘플에서 Positive 클래스에 속한다고 예측한 샘플 수의 비율
        
        $$Recall = \frac{TP}{TP+FN}$$
        
    - **특이도(Specificity)**
        
        실제 Negative 클래스에 속한 샘플에서 Negative 클래스에 속한다고 예측한 샘플 수의 비율 (1-False Positive Rate)
        
        $$Specificity = \frac{TN}{TN+FP}$$
        
    - **F1 Score**
        
        정밀도와 재현율의 조화평균
        
        $$ F1Score = \frac{2*Precision*Recall}{Precision+Recall} $$
        
    - **ROC-AUC**
        
        참긍정률(TPR)과 거짓부정률(FPR) 사이를 표현하기 위한 ROC Curve 곡선하 면적을 의미
        예측된 확률로부터 여러 클래스로 분류를 수행하는데 활용
        
        ![6-2-3](https://github.com/eeeeeddy/Machine_Learning/assets/71869717/2e32d09e-0e13-4767-82be-11c82cf2ca1c)
        
        모든 케이스에 대해 정확히 분류할 경우 (TPR=1, FPR=0) 이며, AUC 면적은 1이 된다.
