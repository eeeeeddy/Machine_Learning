# 데이터 분석 기초

**23.05.23**

### Feature / Target

- **Feature Matrix**
    
    표본(Sample)은 데이터셋이 설명하는 개별 객체를 나타낸다.
    각 표본을 연속적인 수치값, Bool 값, 이산값으로 표현하는 개별 관측치를 의미
    **[n_samples, n_features]** 형태의 2차원 배열 구조 사용
    
- **Target Vector**
    
    Feature Matrix로부터 예측하고자 하는 값의 Vector
    연속적인 수치값, 이산 클래스/레이블을 가짐
    1차원 배열 구조 사용하며, 종속변수, 출력변수, 결과변수, 반응변수라고도 한다.
    
- **Layout**
    
    ![07. 데이터분석 기초.png](/img/데이터분석기초_1.png)
    
    iris 데이터의 경우 **sepal_length, sepal_width, petal_length, petal_width**가 **Feature**,
    **species**가 **Target**이다.
    
    ```python
    import seaborn as sns
    
    iris = sns.load_dataset('iris')
    iris.info()
    ```
    
    ```python
    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 150 entries, 0 to 149
    Data columns (total 5 columns):
     #   Column        Non-Null Count  Dtype  
    ---  ------        --------------  -----  
     0   sepal_length  150 non-null    float64
     1   sepal_width   150 non-null    float64
     2   petal_length  150 non-null    float64
     3   petal_width   150 non-null    float64
     4   species       150 non-null    object 
    dtypes: float64(4), object(1)
    memory usage: 6.0+ KB
    ```
    
    iris 데이터를 분리하여 Feature Matrix와 Target Vector로 분리할 수 있다.
    
    ```python
    X = iris.drop('species', axis=1)
    Y = iris['species']
    ```
    

### Scikit-Learn

- **머신러닝 수행 절차**
    1. 데이터 준비
    2. 모델 클래스 선택
        
        입력 데이터(X), 출력 데이터(y)에 맞는 분석 모델 선택
        
    3. 모델 인스턴스 생성과 하이퍼 파라미터 선택
    4. 특징 행렬과 대상 벡터 준비
    5. 모델을 데이터에 Fit (모델 생성)
        
        ```python
        # 모델 생성
        model.fit(X, y)
        
        # 모델의 기울기
        model.coef_
        
        # 모델의 y 절편
        model.intercept_
        ```
        
    6. 새로운 데이터를 이용해 예측
        
        ```python
        # 새로 입력된 X_new에 대한 모델 예측값(y_pred) 생성
        y_pred = model.predict(X_new)
        ```
        
    7. 모델 평가
        
        ```python
        from sklearn.metrics import mean_squared_error
        from sklearn.metrics import accuracy_score
        
        print(mean_squared_error(y, y_pred))
        print(accuracy_score(y, y_pred))
        ```
        

### Train-Test Split

- **Data Labeling**
    
    학습을 위해서 범주형 변수에 대하여 수치형 변수로 바꾸어주는 작업이 필요하다.
    
    ```python
    from sklearn.preprocessing import LabelEncoder
    
    encoder = LabelEncoder()
    encoder.fit_transform(y)
    ```
    

- **데이터 분할**
    
    머신러딩/딥러닝 학습데이터는 **training data, validation data, test data**로 분할하여 사용
    
    ![Untitled](/img/데이터분석기초_2.png)
    
    - 훈련 데이터 : 모델의 훈련 및 가중치 업데이트 등의 목적으로 사용
    - 검증 데이터 : 훈련된 모델의 평가 및 최종 모델을 선정하기 위해 사용
    - 테스트 데이터 : 모델의 예측 및 평가를 위해 사용
    
    ```python
    from sklearn.model_selection import train_test_split
    
    X_train, X_test, y_train, y_test = 
    										train_test_split(X, y, test_size = 0.2, random_state = 25)
    ```
    

### Cross Validation

- **교차 검증의 절차**
    1. 1단계에서는 데이터를 학습용과 테스트용으로 나눔
    2. 모델의 테스트 성능을 기록
    3. 교차 검증의 매 단계마다 다른 파티션으로 위의 작업을 수행
    4. 모델의 최종 성능은 매 단계의 테스트 성능을 평균 계산
    
    - 교차 검증은 모델의 변동성을 줄여주며 **과적합 방지 효과**
    - 교차 검증을 통해 모든 데이터를 학습용 데이터로 사용할 수 있다.
    
- **K-Fold Cross Validation**
    - 데이터를 무작위로 k개의 동일한 크기인 폴드로 분할
    (일반적으로 k값은 3, 5, 10을 많이 사용)
    - 각 시행 단계에서 특정 폴드를 테스트용으로, 나머지는 학습용으로 사용
    - 각 폴드를 테스트 세트로 한 번씩 사용하고 이 과정을 k번 반복 시행
    - 최종적으로 모델 성능의 평균을 계산
    
    - k=5, repeat=5
        
        ![Untitled](/img/데이터분석기초_3.png)
        
    
    ```python
    from sklearn.model_selection import cross_val_score
    cross_val_score(model, X, y, cv = 5)
    ```
    
    교차 검증을 진행하면서 모델에 학습이 되기 때문에 교차 검증의 결과값(또는 결과값의 평균)은 모델의 성능을 의미한다.
    
- 단일 관측치 제거 방식(LOOCV)
    - Leave-One-Out Cross Validation
    - 매 시행 단계에서 테스트 샘플을 고정하는 방식
    - 데이터를 n개의 서브 세트로 분할하고, 
    n개 중 1개를 테스트용으로 두고 n-1개로 학습을 수행
    - 데이터 크기가 n이면 n번의 교차 검증을 수행
    (검증을 시행할 때 마다 한 지점을 제외한 모든 지점에서 훈련)
        
        ![Untitled](/img/데이터분석기초_4.png)
        
    
    ```python
    from sklearn.model_selection import LeaveOneOut
    cross_val_score(model, X, y, cv = LeaveOneOut())
    ```
    

### Grid Search CV

분류/회귀 알고리즘에 사용되는 Hyper Parameter를 리스트로 입력하여 값에 대해 
각각 예측 성능을 측정, 평가하여 **최적의 Hyper Parameter 값을 찾는 기법**

- **GridSearchCV 클래스의 생성자**
    - estimator : classifier, regressor, pipeline 등
    - param_grid : 튜닝을 위한 파라미터를 dictionary 형태로 만들어서 넣음
    - scoring : 예측 성능을 측정할 평가 방법 설정 (보통 accuracy로 지정)
    - cv : 교차 검증에서의 폴드 수 설정
    - refit : True일 경우 최적의 Hyper Parameter를 찾아서 재학습함. (Default : True)
    
    ```python
    # iris 데이터 이용
    
    params = {'max_depth': [2, 3],
    					'min_samples_split': [2, 3]}
    
    dtc = DecisionTreeClassifier()
    
    grid_tree = GridSearchCV(dtc, param_grid=params, cv=3, refit=True)
    grid_tree.fit(X_train, y_train)
    
    print('Best Parameter : ', grid_tree.best_params_)
    print('Best Score : ', grid_tree.best_score_)
    
    em = grid_tree.best_estimator_
    pred = em.predict(X_val)
    
    print('Accuracy Score : ', accuracy_score(y_val, pred))
    ```
    
    ```python
    Best Parameters :  {'max_depth': 3, 'min_samples_split': 2}
    Best Score :  0.9249999999999999
    Accuracy Score :  0.9666666666666667
    ```
    
- **GridSearch를 이용한 Hyper Parameter 튜닝**
    - 사용자가 지정한 Hyper Parameter 후보군들의 조합 중에서 Best 조합을 탐색
    - Hyper Parameter 후보군의 수에 비례하여 탐색 시간이 증가하는 것이 단점
    
    ```python
    # iris 데이터 이용
    
    dataset = load_iris()
    
    data = dataset['data']
    target = dataset['target']
    
    X_train, X_val, y_train, y_val = 
    		train_test_split(data, target, test_size=0.2, shuffle=True, stratify=target,
    		random_state=34)
    
    # GridSearch가 찾을 parameter 정의
    param_grid = {'n_estimator' : [100, 150, 200, 250],
    							'max_depth' : [None, 6, 9, 12],
    							'min_samples_split' : [0.01, 0.05, 0.1],
    							'max_features' : ['auto', 'sqrt']}
    
    # 적용할 estimator(모델) 정의
    estimator = RandomForestRegressor()
    
    from sklearn.model_selection import KFold
    
    kf = KFold(random_state=30, n_splits=10, shuffle=True)
    
    # GridSearch 실행
    # n_jobs=-1로 지정해주면 모든 코어를 다 사용하기에 처리 속도 증가
    # verbose로 log 출력의 level을 조정 (숫자가 클 수록 많은 log 출력) 가능
    grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=kf, 
    													 n_jobs=-1, verbose=2)
    grid_search.fit(X_train, y_train)
    
    # Best Parameter 결과 확인
    grid_search.best_params_
    ```
    
    ```python
    {'max_depth': 6,
     'max_features': 'sqrt',
     'min_samples_split': 0.1,
     'n_estimators': 200}
    ```