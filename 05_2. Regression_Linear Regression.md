### Linear Regression

- **LinearRegression 클래스**
    
    예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화하여
    OLS(Ordinary Least Squares) 추정 방식으로 구현한 클래스
    
    **fit( )** 메서드로 X, y 배열을 입력받으면 회귀 계수(Coefficients)인 W를 coef_ 속성에 저장
    
    ```python
    class sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)
    ```
    
    ![선형회귀_1.png](/img/5-2-1.png)
    
    OLS 기반의 회귀 계수 계산은 입력 피처의 독립성에 많은 영향을 받는다.
    피처간의 상관 관계가 매우 높은 경우 분산이 매우 커져서 오류에 매우 민감해진다.
    (이런 현상을 **다중공선성 문제**라고 한다.)
    이러한 경우, **1) 독립적인 중요한 피처만 남기고 제거 또는 규제를 적용**하거나 
    **2) PCA를 통해 차원 축소를 수행**하는 방법을 고려해 볼 수 있다.
    
- **회귀 평가 지표**
    - **MAE (Mean Absolute Error)**
        
        실제 값과 예측값의 차이를 절댓값으로 변환해 평균 연산
        
    - **MSE (Mean Squared Error)**
        
        실제 값과 예측값의 차이를 제곱하여 평균 연산
        
    - **RMSE**
        
        MSE 값은 오류의 제곱을 구하므로 실제 오류 평균보다 더 커지는 특성이 있으므로
        MSE의 제곱근을 구한 것
        
    - **R^2**
        
        분산 기반으로 예측 성능을 평가
        실제 값의 분산 대비 예측값의 분산 비율을 지표로 하여, 1에 가까울수록 예측 정확도가 높다.
        
    
    | 평가 방법 | 사이킷런 평가 지표 API | Scoring 함수 적용 값 |
    | --- | --- | --- |
    | MAE | metrics.mean_absolute_error | ‘neg_mean_absolute_error’ |
    | MSE | metrics.mean_squared_error | ‘neg_mean_squared_error’ |
    | RMSE | metrics.mean_squared_error(squared = False) | ‘neg_root_mean_squared_error’ |
    | MSLE | metrics.mean_squared_log_error | ‘neg_mean_squared_log_error’ |
    | R^2 | metrics.r2_score | ‘r2’ |
    
    실제 값과 예측값의 오류 차이를 기반으로 하는 회귀 평가 지표의 경우 값이 커지면
    성능이 좋지 않은 모델을 의미
    
- **LinearRegression을 이용한 보스턴 주택 가격 회귀 구현**
    
    
    | CRIM | 지역별 범죄 발생률 |
    | --- | --- |
    | ZN | 25,000평방피트를 초과하는 거주 지역의 비율 |
    | INDUS | 비상업 지역 넓이 비율 |
    | CHAS | 찰스강에 대한 더미 변수(강의 경계에 위치한 경우는 1, 아니면 0) |
    | NOX | 일산화질소 농도 |
    | RM | 거주할 수 있는 방 개수 |
    | AGE | 1940년 이전에 건축된 소유 주택의 비율 |
    | DIS | 5개 주요 고용센터까지의 가중 거리 |
    | RAD | 고속도로 접근 용이도 |
    | TAX | 10,000달러당 재산세율 |
    | PTRATIO | 지역의 교사와 학생 수 비율 |
    | B | 지역의 흑인 거주 비율 |
    | LSTAT | 하위 계층의 비율 |
    | MEDV | 본인 소유의 주택 가격(중앙값) |
    
    ```python
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.datasets import load_boston # 1.2 버전에서 삭제 예정
    from scipy import stats
    
    boston = load_boston()
    df = pd.DataFrame(boston.data, columns = boston.feature_names)
    df['PRICE'] = boston.target
    ```
    
    ```python
    # 각 컬럼별로 주택 가격에 미치는 영향도 조사
    fig, axs = plt.subplot(figsize=(16, 8), ncols=4, nrows=2)
    lm_features = ['RM', 'ZN', 'INDUS', 'NOX', 'AGE', 'PTRATIO', 'LSTAT', 'RAD']
    for i, feature in enumerate(lm_features):
    	row = int(i/4)
    	col = i%4
    	# seaborn의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현
    	sns.regplot(x=feature, y='PRICE', data=df, ax=axs[row][col])
    ```
    
    ![output.png](/img/5-2-2.png)
    
    다른 컬럼보다 RM과 LSTAT의 PRICE 영향도가 가장 두드러지게 나타남을 볼 수 있다.
    
     
    
    ```python
    # 회귀 모델 생성
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error, r2_score
    
    y_target = df['PRICE']
    X_data = df.drop(['PRICE'], axis=1, inplace=False)
    
    X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.3, random_state=156)
    
    # 선형 회귀 OLS로 학습/예측/평가 수행
    lr = LinearRegression()
    lr.fit(X_train, y_train)
    y_preds = lr.predict(X_test)
    mse = mean_squared_error(y_test, y_preds)
    rmse = np.sqrt(mse)
    
    print('MSE : {0:.3f}, RMSE:{1:.3f}'.format(mse, rmse))
    print('Variance score : {0:.3f}'.format(r2_score(y_test, y_preds)))
    print('절편 값:', lr.intercept_)
    print('회귀 계수 값:', np.round(lr.coef_, 1))
    ```
    
    ```python
    MSE : 13.483 , RMSE : 3.672
    Variance score : 0.811
    절편 값: 42.277854302723185
    회귀 계수값: 
    [-0.1  -0.  0.1  2.4  -16.6  0.7  -0.  -0.9  0.2  -0.  -0.5  0.  -0.5  13.]
    ```
    
    ```python
    # 회귀 계수를 큰 값 순으로 정렬하기 위해 Series로 생성. index 컬럼명에 유의
    coeff = pd.Series(data = np.round(lr.coef_, 1), index = X_data.columns)
    coeff.sort_values(ascending = False)
    ```
    
    ```python
    CAT. MEDV    13.0
    CHAS          2.4
    RM            0.7
    RAD           0.2
    INDUS         0.1
    ZN           -0.0
    AGE          -0.0
    TAX          -0.0
    B             0.0
    CRIM         -0.1
    PTRATIO      -0.5
    LSTAT        -0.5
    DIS          -0.9
    NOX         -16.6
    dtype: float64
    ```
    
    ```python
    from sklearn.model_selection import cross_val_score
    
    y_target = df['MEDV']
    X_data = df.drop(['MEDV'], axis=1, inplace=False)
    
    # cross_val_score()로 5 Fold 세트로 MSE를 구한 뒤 이를 기반으로 다시 RMSE를 구함
    neg_mse_scores = cross_val_score(lr, X_data, y_target, scoring="neg_mean_squared_error", cv=5)
    rmse_score = np.sqrt(-1 * neg_mean_scores)
    avg_rmse = np.mean(rmse_scores)
    
    # cross_val_score(scoring="neg_mean_squared_error")로 반환된 값은 모두 음수
    print('5 folds의 개별 Negative MSE scores:', np.round(neg_mse_scores, 2))
    print('5 folds의 개별 RMSE scores:', np.round(rmse_scores, 2))
    print('5 folds의 평균 RMSE : {0:.3f}'.format(avg_rmse))
    ```
    
    ```python
    5 folds의 개별 Negative MSE scores: [ -6.54 -15.62 -23.68 -44.52 -16.45]
    5 folds의 개별 RMSE scores : [2.56 3.95 4.87 6.67 4.06]
    5 folds의 평균 RMSE : 4.421
    ```
