### Regression Tree

회귀를 위한 트리를 생성하고 이를 기반으로 회귀 예측을 하는 것
분류 트리와 크게 다르지 않으나 리프 노드에서 예측 결정 값을 만드는 과정에서 차이가 있다.
분류 트리가 특정 클래스 레이블을 결정하는 것과 달리 회귀 트리는 리프 노드에 속한 
데이터 값의 평균값을 구해 회귀 예측값을 계산한다.

X 값의 균일도를 반영한 지니 계수에 따라 X 피처를 결정 트리 기반으로 분할한다. 
이후 재귀적으로 분할하여 트리를 생성한다.

![그림01.png](/img/5-6-1.png)

리프 노드 생성 기준에 부합하는 트리 분할이 완료됐다면 리프 노드에 소속된 데이터 값의
평균값을 구해서 최종적으로 리프 노드에 결정 값으로 할당한다.

![그림02.png](/img/5-6-2.png)

- **RandomForestRegressor를 이용하여 보스턴 주택 가격 예측**
    
    ```python
    from sklearn.datasets import load_boston
    from sklearn.model_selection import cross_val_score
    from sklearn.ensemble import RandomForestRegressor
    import pandas as pd
    import numpy as np
    
    # 보스턴 데이터 세트 로드
    boston = load_boston()
    df = pd.DataFrame(boston.data, columns=boston.feature_names)
    
    df['PRICE'] = boston.target
    y_target = df['PRICE']
    X_data = df.drop(['PRICE'], axis=1, inplace=False)
    
    rf = RandomForestRegressor(random_state=0, n_estimators=1000)
    neg_mse_scores = 
     cross_val_score(rf, X_data, y_target, scoring='neg_mean_squared_error', cv=5)
    rmse_scores = nq.sqrt(-1 * neg_mse_scores)
    avg_rmse = np.mean(rmse_scores)
    
    print('5 교차 검증의 개별 Negative MSE scores:', np.round(neg_mse_scores, 2))
    print('5 교차 검증의 개별 RMSE scores:', np.round(rmse_scores, 2))
    print('5 교차 검증의 평균 RMSE: {0:.3f}'.format(avg_rmse))
    ```
    
    ```python
    5 교차 검증의 개별 Negative MSE scores:  [ -6.85  -9.01 -13.83 -16.6  -14.09]
    5 교차 검증의 개별 RMSE scores :  [2.62 3.   3.72 4.07 3.75]
    5 교차 검증의 평균 RMSE : 3.433
    ```
    
    여러 유형의 회귀트리를 이용해 보스턴 주택 가격 예측을 수행하기 위해 함수 생성
    
    ```python
    def get_model_cv_prediction(model, X_data, y_target):
    	neg_mse_scores = cross_val_score(model, X_data, y_target, 
    																	 scoring='neg_mean_squared_error', cv=5)
    	rmse_scores = np.sqrt(-1 * neg_mse_scores)
    	avg_rmse = np.mean(rmse_scores)
    	print('## ', model.__class__.__name__, ' ##')
    	print(' 5 교차 검증의 평균 RMSE : {0:.3f}'.format(avg_rmse))
    ```
    
    ```python
    from sklearn.tree import DecisionTreeRegressor
    from sklearn.ensemble import GradientBoostingRegressor
    from xgboost import XGBRegressor
    from lightbgm import LGBMRegressor
    
    dt_reg = DecisionTreeRegressor(random_state=0, max_depth=4)
    rf_reg = RandomForestRegressor(random_state=0, n_estimators=1000)
    gb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000)
    xgb_reg = XGBRegressor(n_estimators=1000)
    lgb_reg = LGBMRegressor(n_estimators=1000)
    
    # 트리 기반의 회귀 모델을 반복하면서 평가 수행
    models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg]
    for model in models:
    	get_model_cv_prediction(model, X_data, y_target)
    ```
    
    ```python
    ##  DecisionTreeRegressor  ##
     5 교차 검증의 평균 RMSE : 3.885 
    ##  RandomForestRegressor  ##
     5 교차 검증의 평균 RMSE : 3.433 
    ##  GradientBoostingRegressor  ##
     5 교차 검증의 평균 RMSE : 3.537 
    ##  XGBRegressor  ##
     5 교차 검증의 평균 RMSE : 3.832 
    ##  LGBMRegressor  ##
     5 교차 검증의 평균 RMSE : 3.885
    ```
    
    **feature_importances_** 를 이용한 보스턴 주택 가격 모델의 피처별 중요도 시각화
    
    ```python
    import seaborn as sns
    
    rf_reg = RandomForestRegressor(n_estimators=1000)
    
    rf_reg.fit(X_data, y_target)
    
    feature_series = pd.Series(data=rf_reg.featrue_importances_, index=X_data.columns)
    feature_series = feature_series.sort_values(ascending=False)
    sns.barplot(x = feature_series, y = feature_series.index)
    ```
    
    ![output.png](/img/5-6-3.png)
    
    회귀 트리 Regressor가 어떻게 예측값을 판단하는지 선형 회귀와 비교하여 시각화 <br>
    (결정 트리의 하이퍼 파라미터인 max_depth의 크기를 변화시키면서 확인)
    
    ```python
    df_sample = df[['RM', 'MEDV']]
    df_sample = df_sample.sample(n=100, random_state=0)
    print(df_sample.shape)
    plt.figure()
    plt.scatter(df_sample.RM, df_sample.PRICE, c='darkorange')
    ```
    
    ```python
    (100, 2)
    ```
    
    ![output1.png](/img/5-6-4.png)
    
    ```python
    import numpy as np
    from sklearn.linear_model import LinearRegression
    
    # 선형 회귀와 결정 트리 기반의 Regressor 생성
    # DecisionTreeRegressor의 max_depth는 각각 2, 7
    lr_reg = LinearRegression()
    rf_reg2 = DecisionTreeRegressor(max_depth=2)
    rf_reg7 = DecisionTreeRegressor(max_depth=7)
    
    # 실제 예측을 적용할 테스트용 데이터 세트를 4.5~8.5까지의 100개 데이터 세트로 생성
    X_test = np.arange(4.5, 8.5, 0.04).reshape(-1, 1)
    
    # 보스턴 주택 가격 데이터에서 시각화를 위해 피처는 RM만, 결정데이터는 PRICE 추출
    X_feature = df_sample['RM'].values.reshape(-1, 1)
    y_target = df_sample['MEDV'].values.reshape(-1, 1)
    
    # 학습과 예측 수행
    lr_reg.fit(X_feature, y_target)
    rf_reg2.fit(X_feature, y_target)
    rf_reg7.fit(X_feature, y_target)
    
    pred_lr = lr_reg.predict(X_test)
    pred_rf2 = rf_reg2.predict(X_test)
    pred_rf7 = rf_reg7.predict(X_test)
    ```
    
    **학습된 Regressor(lr_reg / rf_reg2 / rf_reg7)**에서 예측한 PRICE 회귀선 시각화
    
    ```python
    fig, (ax1, ax2, ax3) = plt.subplot(figsize=(14, 4), ncols=3)
    
    # X축 값을 4.5~8.5로 변환하여 입력했을 때 선형 회귀와 결정 트리 회귀 예측선 시각화
    
    # 선형 회귀로 학습된 모델 회귀 예측선
    ax1.set_title('Linear Regression')
    ax1.scatter(df_sample.RM, df_sample.MEDV, c='darkorange')
    ax1.plot(X_test, pred_lr, label='linear', linewidth=2)
    
    # DecisionTreeRegressor의 max_depth를 2로 했을 때 회귀 예측선
    ax2.set_title('Decision Tree Regressor: \n max_depth=2')
    ax2.scatter(df_sample.RM, df_sample.MEDV, c='darkorange')
    ax2.plot(X_test, pred_rf2, label='max_depth:2', linewidth=2)
    
    # DecisionTreeRegressor의 max_depth를 7로 했을 때 회귀 예측선
    ax3.set_title('Decision Tree Regressor: \n max_depth=7')
    ax3.scatter(df_sample.RM, df_sample.MEDV, c='darkorange')
    ax3.plot(X_test, pred_rf7, label='max_depth:7', linewidth=2)
    ```
    
    ![output2.png](/img/5-6-5.png)
    
    **선형회귀**는 예측 회귀선을 직선으로 표현
    
    **회귀 트리**의 경우 분할되는 데이터 지점에 따라 브랜치를 만들면서 계단 형태로 회귀선 생성 <br>
    (max_depth가 7인 경우에는 학습 데이터 세트의 이상치 데이터도 학습하면서 복잡한 계단
    형태의 회귀선을 생성하여 과적합이 되기 쉬운 모델임을 알 수 있다.)