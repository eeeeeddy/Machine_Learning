### Logistic Regression

선형 회귀 방식을 분류에 적용한 알고리즘
회귀의 선형/비선형 여부는 독립 변수가 아닌 가중치(weight) 변수의 선형/비선형에 따른다.

로지스틱 회귀가 선형 회귀와 다른 점은 학습을 통해 선형 회귀 함수의 회귀 최적선을 찾는 것이 아니라 Sigmoid 함수 최적선을 찾고 이 Sigmoid 함수의 반환 값을 확률로 간주해 확률에 따라 
분류를 결정

- **Sigmoid Function**
    
    $$
    y = \frac{1}{1+e^{-x}}
    $$
    
- **회귀 계수 최적화**
    - **lbfgs**
        
        사이킷런 버전 0.22부터 solver의 기본 설정 값으로 메모리 공간을 절약할 수 있고,
        CPU 코어 수가 많다면 최적화를 병렬로 수행할 수 있다.
        
    - **liblinear**
        
        사이킷런 버전 0.21까지 solver의 기본 설정 값으로 다차원이고 작은 데이터 세트에서
        효과적으로 동작하지만 국소 최적화 이슈가 있고, 병렬로 최적화할 수 없다.
        
    - **newton-cg**
        
        좀 더 정교한 최적화를 가능하게 하지만, 대용량의 데이터에서 속도가 많이 느리다.
        
    - **sag**
        
        Stochastic Average Gradient로 경사하강법 기반의 최적화를 적용한다. 
        대용량의 데이터에서 빠르게 최적화한다.
        
    - **saga**
        
        sag와 유사한 최적화 방식이며, L1 정규화를 가능하게 한다.
        
- **Logistic Regression 예제 (위스콘신 유방암 데이터 세트)**
    
    ```python
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_breast_cancer
    from sklearn.linear_model import LogisticRegression
    
    df = load_breast_cancer()
    ```
    
    ```python
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    
    # StandardScaler()로 평균이 0, 분산 1로 데이터 분포도 변환
    scaler = StandardScaler()
    data_scaled = scaler.fit_transform(df.data)
    
    X_train, X_test, y_train, y_test = train_test_split(data_scaled, df.target, test_size=0.3, random_state=0)
    ```
    
    ```python
    from sklearn.metrics import accuracy_score, roc_auc_score
    
    # 로지스틱 회귀를 이용하여 학습 및 예측 수행
    # solver 인자값을 생성자로 입력하지 않으면 solver='lbfgs'
    lr_clf = LogisticRegression()
    lr_clf.fit(X_train, y_train)
    lr_preds = lr_clf.predict(X_test)
    
    # accuracy와 roc_auc 측정
    print('Accuracy: {0:.3f}'.format(accuracy_score(y_test, lr_preds)))
    print('ROC_AUC: {0:.3f}'.format(roc_auc_score(y_test, lr_preds)))
    ```
    
    ```python
    Accuracy: 0.977
    ROC_AUC: 0.972
    ```
    
    서로 다른 **solver 값**으로 **Logistic Regression 학습** 후 성능 평가
    
    ```python
    solvers = ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']
    
    # 여러 개의 solver값별로 LogisticRegression 학습 후 성능 평가
    for solver in solvers:
    	lr_clf = LogisticRegression(solver=solver, max_iter=600)
    	lr_clf.fit(X_train, y_train)
    	lr_preds = lr_clf.predict(X_test)
    
    	# accuracy와 roc_auc 측정
    	print('solver: {0}, Accuracy: {1:.3f}'.format(solver, accuracy_score(y_test, lr_preds)))
    	print('solver: {0}, ROC_AUC: {1:.3f}'.format(solver, roc_auc_score(y_test, lr_preds)))
    ```
    
    ```python
    solver: lbfgs, Accuracy: 0.977
    solver: lbfgs, ROC_AUC: 0.972
    solver: liblinear, Accuracy: 0.982
    solver: liblinear, ROC_AUC: 0.979
    solver: newton-cg, Accuracy: 0.977
    solver: newton-cg, ROC_AUC: 0.972
    solver: sag, Accuracy: 0.982
    solver: sag, ROC_AUC: 0.979
    solver: saga, Accuracy: 0.982
    solver: saga, ROC_AUC: 0.979
    ```
    
    **GridSearchCV**를 이용하여 **solver, penalty, C** 최적화
    
    ```python
    from sklearn.model_selection import GridSearchCV
    
    params = {'solver' : ['liblinear', 'lbfgs'], 'penalty' : ['l2', 'l1'], 'C' : [0.01, 0.1, 1, 5, 10]}
    
    lr_clf = LogisticRegression()
    
    grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring='accuracy', cv=3)
    grid_clf.fit(data_scaled, df.target)
    print('최적 하이퍼 파라미터: {0}, 최적 평균 정확도: {1:.3f}'.format(grid-clf.best_params_, grid_clf.best_score_))
    ```
    
    ```python
    최적 하이퍼 파라미터: {'C': 1, 'penalty': 'l2', 'solver': 'lib}, 최적 평균 정확도: 0.975
    ```
