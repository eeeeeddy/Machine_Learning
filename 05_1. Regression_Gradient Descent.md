### Gradient Descent

비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 뛰어난 방식

점진적으로 반복적인 계산을 통해 W 파라미터 값을 업데이트하면서 오류 값이 최소가 되도록
W 파라미터를 구하는 방식

최초 W에서 미분을 적용한 뒤, 미분값이 계속 감소하는 방향으로 순차적으로 W값을 업데이트

미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용함수가 최소인 지점으로 간주하고 
그 때의 w를 반환

![그림01.png](/img/5-1-1.png)

- **경사하강법 Process**
    1. w1, w0를 임의의 값으로 설정하고 첫 비용 함수의 값을 계산
    2. w1, w0의 값을 편미분 값으로 업데이트한 후 다시 비용 함수의 값을 계산
    3. 비용 함수의 값이 감소했다면 다시 ‘2번’을 반복하면서 더이상 비용 함수의 값이
    감소하지 않으면 그 때의 w1, w0를 구하고 반복을 중지
    
    **용어 정리**
    
    - **Epoch**
        
        모든 훈련 데이터셋을 학습하는 횟수로 너무 많은 Epoch는 과적합의 위험이 있다.
        
    - **Batch Size**
        
        훈련 데이터셋 중 몇 개의 데이터를 묶어서 가중치 값을 갱신할 것인지에 대한 정보
        훈련 데이터셋의 개수가 1000개, 1 Epoch, Batch Size가 100일 경우 → 10번의 갱신
        
        Batch Size는 메모리에 적재시킬 수 있을 만큼의 Size로 정하는게 좋으며,
        너무 작게 설정할 경우 Iteration이 증가하여 학습시간이 오래 걸릴 수 있다.
        
    - **Iteration**
        
        한 Epoch를 진행하기 위해, 몇 번의 가중치 갱신이 이루어지는지에 대한 정보
        
    
    ```python
    # 회귀식 y=4x+6을 근사하기 위한 100개의 데이터 세트 생성 후,
    # 경사하강법을 이용해 회귀 계수 w1, w0 도출하기
    
    import numpy as np
    import pandas as pd
    
    np.random.seed(0)
    # y=4x+6을 근사(w1=4, w0=6) 임의의 값은 노이즈를 위해 생성
    X = 2 * np.random.rand(100, 1)
    y = 6 + 4 * X + np.random.rand(100, 1)
    ```
    
    **get_weight_updates( ) 함수**
    
    입력 배열 X 값에 대한 예측 배열 y_pred는 **np.dot(X, w1.T) + w0**로 구할 수 있다.
    100개의 데이터 X(1, 2, …, 100)가 있다면 예측값은 **w0 + X(1)*w1 + … + X(100)*w1**이며, 
    이는 입력 배열 X와 w1 배열의 내적과 동일하다.
    
    ```python
    # w1과 w0을 업데이트할 w1_update, w0_update를 반환
    def get_weight_updates(w1, w0, X, y, learning_rate=0.01):
    	N = len(y)
    	
    	# w1_update, w0_update를 각각 w1, w0의 shape와 동일한 크기를 가진 0값으로 초기화
    	w1_update = np.zeros_like(w1)
    	w0_update = np.zeros_like(w0)
    	
    	# 예측 배열을 계산하고 예측과 실제 값의 차이 계산
    	y_pred = np.dot(X, w1.T) + w0
    	diff = y - y_pred
    
    	# w0_update를 dot 행렬 연산으로 구하기 위해 모두 1값을 가진 행렬 생성
    	w0_factors = np.ones((N, 1))
    
    	# w1과 w0을 업데이트할 w1_update와 w0_update 계산
    	w1_update = -(2/N) * learning_rate * (np.dot(X.T, diff))
    	w0_update = -(2/N) * learning_rate * (np.dot(w0_factors.T, diff))
    
    	retrun w1_update, w0_update
    ```
    
    **gradient_descent_steps( ) 함수**
    
    get_weight_updates( )를 경사하강 방식으로 반복 수행하여 w1과 w0를 업데이트하는 함수
    
    ```python
    # 입력 인자 iters로 주어진 횟수만큼 반복적으로 w1, w0을 업데이트 적용함
    def gradient_descent_steps(X, y, iters=10000):
    	# w0, w1을 0으로 초기화
    	w0 = np.zeros((1, 1))
    	w1 = np.zeros((1, 1))
    
    	# iters 만큼 반복적으로 get_weight_updates() 호출하여 w1, w2 업데이트 수행
    	for ind in range(iters):
    		w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01)
    		w1 -= w1_update
    		w0 -= w0_update
    
    	return w1, w0
    ```
    
    **get_cost( ) 함수**
    
    예측 오차 비용 계산을 수행하는 함수
    
    ```python
    # 비용 함수 정의
    # 실제 y값과 예측된 y값을 인자로 계산
    def get_cost(y, y_pred):
    	N = len(y)
    	cost = np.sum(np.square(y-y_pred))/N
    	return cost
    ```
    
    **경사하강법 수행**
    
    ```python
    w1, w0 = gradient_descent_steps(X, y, iters=1000)
    print("w1: {0:.3f} w0: {1:.3f}".format(w1[0,0], w0[0,0]))
    
    y_pred = w1[0,0] * X + w0
    print('Gradient Descent Total Cost: {0:.4f}'.format(get_cost(y, y_pred)))
    ```
    
    ```python
    w1:4.022 w0:6.162
    Gradient Descent Total Cost:0.9935
    ```
    
    일반적으로 경사하강법은 모든 학습 데이터에 대해 반복적으로 비용함수 최소화를 위한
    값을 업데이트하기 때문에 수행 시간이 매우 오래 걸림
    따라서 실전에서는 대부분 **확률적 경사 하강법(Stochastic Gradient Descent)** 을 이용
    
    **확률적 경사하강법**
    
    전체 입력 데이터가 아닌 일부 데이터만을 이용하여 w가 업데이트되는 값을 계산
    
    ```python
    # 미니 배치 확률적 경사 하강법
    def stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000):
    	w0 = np.zeros((1, 1))
    	w1 = np.zeros((1, 1))
    
    	for ind in range(iters):
    		np.random.seed(ind)
    		
    		# 전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이터를 추출하여
    		# sample_X, sample_y로 저장
    		stochastic_random_index = np.random.permutation(X.shape[0])
    		sample_X = X[stochastic_random_index[0:batch_size]]
    		sample_y = y[stochastic_random_index[0:batch_size]]
    
    		# 랜덤하게 batch_size만큼 추출된 데이터를 기반으로 
    		# w1_update, w0_update 계산 후 업데이트
    		w1_update, w0_update 
    				= get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01)
    		w1 -= w1_update
    		w0 -= w0_update
    
    	return w1, w0
    ```
    
    ```python
    # 확률적 경사하강법을 이용하여 w1, w0 및 예측 오류 비용 계산
    w1, w0 = stochastic_gradient_descent_steps(X, y, iters=1000)
    print("w1:", round(w1[0,0],3), "w0:", round(w0[0,0],3))
    
    y_pred = w1[0,0] * X + w0
    print(
    	'Stochastic Gradient Descent Total Cost: {0:.4f}'.format(get_cost(y, y_pred)))
    ```
    
    ```python
    w1: 4.028 w0: 6.156
    Stochastic Gradient Descent Total Cost:0.9937
    ```
    
    확률적 경사하강법의 결과와 경사하강법의 결과는 큰 차이가 없음을 알 수 있다.
    따라서 큰 데이터를 처리할 경우에는 경사하강법은 매우 시간이 오래 걸리므로 일반적으로 확률적 경사하강법을 사용한다.