### Polynomial Regression

- **다항 회귀 이해**
    
    독립변수의 단항식이 아닌 2차, 3차 방정식과 같은 다항식으로 표현되는 것
    
    $$y = w_0 + w_1*x_1 + w_2*x_2 + w_3*x_1*x_2 + w_4*x_1^2 + w_5*w_2^2$$
    
    다항회귀는 선형회귀이며, 회귀에서 선형/비선형을 나누는 기준은 회귀 계수가
    선형/비선형의 여부이다.
    
    다만 **Sklearn**에서는 다항 회귀를 위한 클래스를 제공하지 않으므로, 비선형 함수를
    선형 모델에 적용시키는 방법을 사용해 구현한다.
    
    **Sklearn**의 **PolynomialFeatures** 클래스를 통해 피처를 Polynomial(다항식) 피처로 변환
    
    ```python
    from sklearn.preprocessing import PolynomialFeatures
    import numpy as np
    
    # 다항식으로 변환할 단항식 생성, [[0,1],[2,3]]의 2X2 행렬 생성
    X = np.arange(4).reshape(2, 2)
    print('일차 단항식 계수 피처:\n', X)
    
    # degree=2인 2차 다항식으로 변환하기 위해 PolynomialFeatures를 이용해 변환
    poly = PolynomialFeatures(degree=2)
    poly.fit(X)
    poly_ftr = poly.transform(X)
    print('변환된 2차 다항식 계수 피처:\n', poly_ftr)
    ```
    
    ```python
    일차 단항식 계수 feature:
     [[0 1]
     [2 3]]
    변환된 2차 다항식 계수 feature:
     [[1. 0. 1. 0. 0. 1.]
     [1. 2. 3. 4. 6. 9.]]
    ```
    
    3차 다항 계수를 이용해 3차 다항 회귀 함수식 유도 <br>
    (3차 다항 회귀 결정 함수식 : $y = 1 + 2x_1 + 3x_1^2 + 4x_2^3$ )
    
    ```python
    def polynomial_func(X):
    	y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3
    	return y
    
    X = np.arange(4).reshape(2,2)
    print('일차 단항식 계수 feature:\n', X)
    y = polynomial_func(X)
    print('삼차 다항식 결정값:\n', y)
    ```
    
    ```python
    일차 단항식 계수 feature: 
     [[0 1]
     [2 3]]
    삼차 다항식 결정값: 
     [  5 125]
    ```
    
    3차 다항식 계수의 피처값과 3차 다항식 결정값으로 학습
    
    ```python
    # 3차 다항식 변환
    poly_ftr = PolynomialFeatures(degree=3).fit_transform(X)
    print('3차 다항식 계수 feature:\n', poly_ftr)
    
    # Linear Regression에 3차 다항식 계수 feature와 3차 다항식 결정값으로 학습 후 
    # 회귀 계수 확인
    model = LinearRegression()
    model.fit(poly_ftr, y)
    print('Polynomial 회귀 계수\n', np.round(model.coef_, 2))
    print('Polynomial 회귀 Shape:', model.coef_.shape)
    ```
    
    ```python
    3차 다항식 계수 feature: 
     [[ 1.  0.  1.  0.  0.  1.  0.  0.  0.  1.]
     [ 1.  2.  3.  4.  6.  9.  8. 12. 18. 27.]]
    Polynomial 회귀 계수
     [0.   0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34]
    Polynomial 회귀 Shape : (10,)
    ```
    
    **Sklearn**의 **Pipeline 객체**를 이용하여 한번에 다항 회귀를 구현하기
    
    ```python
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    from sklearn.pipeline import Pipeline
    import numpy as np
    
    def polynomial_func(X):
    	y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3 
      return y
    
    # Pipeline 객체로 Streamline하게 Polynomial Feature 변환과 Linear Regression을 연결
    model = Pipeline([('poly', PolynomialFeatures(degree=3)), ('linear', LinearRegression())])
    X = np.arange(4).reshape(2,2)
    y = polynomial_func(X)
    
    model = model.fit(X, y)
    
    print('Polynomial 회귀 계수\n', np.round(model.named_steps['linear'].coef_, 2))
    ```
    
    ```python
    Polynomial 회귀 계수
    [0.   0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34]
    ```
    

- **다항 회귀를 이용한 Overfitting, Underfitting 이해**
    
    다항식의 차수가 높아질수록 매우 복잡한 피처 간의 관계까지 모델링이 가능하다.
    하지만 다항 회귀의 차수(degree)를 높일수록 학습 데이터에만 너무 맞춘 학습이 이루어져
    정작 테스트 데이터 환경에서는 오히려 예측 정확도가 떨어진다.
    즉, 차수가 높아질수록 과적합의 문제가 크게 발생한다.
    
    Feature X와 target Y가 Noise가 포함된 다항식의 Cosine 그래프 관계에 기반하여 
    다항 회귀의 차수를 변화시키면서 그에 따른 회귀 예측 곡선과 예측 정확도 비교
    
    ```python
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.linear_model import LinearRegression
    from sklearn.model_selection import cross_val_score
    
    # random값으로 구성된 X값에 대해 Cosine 변환
    def true_fun(X):
    	return np.cos(1.5 * np.pi * X)
    
    # X는 0부터 1까지 30개의 random값을 순서대로 sampling한 데이터
    np.random.seed(0)
    n_samples = 30
    X = np.sort(np.random.rand(n_samples))
    
    # y값은 Cosine 기반의 true_fun()에서 약간의 Noise 변동 값을 더한 값
    y = true_fun(X) + np.random.randn(n_samples) * 0.1
    ```
    
    ```python
    # 다항 회귀의 차수(degree)를 1, 4, 15로 각각 변경하면서 예측 결과 비교
    plt.figure(figsize=(14, 5))
    degrees = [1, 4, 15]
    
    for i in range(len(degrees)):
    	ax = plt.subplot(1, len(degrees), i+1)
    	plt.setp(ax, xticks=(), yticks=())
    
    	# degree 별로 각각 Polynomial 변환
    	polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)
    	linear_regression = LinearRegression()
    	pipeline = Pipeline([("polynomial_featrues", polynomial_features), ("linear_regression", linear_regression)])
    	pipeline.fit(X.reshape(-1, 1), y)
    
    	# 교차 검증으로 다항 회귀를 평가
    	scores = cross_val_score(pipeline, X.reshape(-1,1), y, scoring="neg_mean_squared_error", cv=10)
    	coefficients = pipeline.named_steps['linear_regression'].coef_
    	print('\nDegree {0} 회귀 계수는 {1} 입니다.'.format(degrees[i], np.round(coefficients, 2)))
    	print('Degree {0} MSE는 {1:.2f} 입니다.'.format(degrees[i], -1*np.mean(scores)))
    	
    	# 0부터 1까지 테스트 데이터 세트를 100개로 나누어 예측을 수행
    	# 테스트 데이터 세트에 회귀 예측을 수행하고, 예측 곡선과 실제 곡선을 그려 비교
    	X_test = np.linspace(0, 1, 100)
    	# 예측값 곡선
    	plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model")
    	# 실제값 곡선
    	plt.plot(X, y, edgecolor='b', s=20, label="Samples")
    
    	plt.xlabel("X")
    	plt.ylabel("y")
    	plt.xlim((0, 1))
    	plt.ylim((-2, 2))
    	plt.legend(loc="best")
    	plt.title("Degree {}\nMSE = {:.2e}(+/- {:.2e})".format(degrees[i], -scores.mean(), scores.std()))
    
    plt.show()
    ```
    
    ```python
    Degree 1 회귀 계수는 [-2.] 입니다.
    Degree 1 MSE 는 0.41 입니다.
    
    Degree 4 회귀 계수는 [  0. -18.  24.  -7.] 입니다.
    Degree 4 MSE 는 0.04 입니다.
    
    Degree 15 회귀 계수는 [-2.98300000e+03  1.03900000e+05 -1.87417100e+06  2.03717220e+07
     -1.44873987e+08  7.09318780e+08 -2.47066977e+09  6.24564048e+09
     -1.15677067e+10  1.56895696e+10 -1.54006776e+10  1.06457788e+10
     -4.91379977e+09  1.35920330e+09 -1.70381654e+08] 입니다.
    Degree 15 MSE 는 182815433.48 입니다.
    ```
    
    ![output1.png](/img/5-3-1.png)
    
    **Degree 4**가 가장 뛰어난 예측 성능을 나타내며, **Degree 15**는 학습 데이터 세트만 정확하게 예측하고, 테스트 값의 실제 곡선과는 완전히 다른 형태의 예측 곡선이 생성되었다.
    
    좋은 예측 모델은 **Degree 1**과 같이 학습 데이터의 패턴을 지나치게 단순화한 과소적합 모델도 아닌, **Degree 15**와 같이 모든 학습 데이터의 패턴을 하나하나 감안한 지나치게 복잡한 과적합 모델도 아닌 **Degree 4**와 같이 학습 데이터의 패턴을 잘 반영하면서도 복잡하지 않은 균형잡힌 모델을 의미한다.
