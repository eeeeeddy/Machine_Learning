### Ensemble

- **Ensemble**
    - 여러 개의 분류기를 생성하고 그 예측을 결합함으로써 보다 정확하고 신뢰성이 
    높은 최종 예측을 도출하는 기법
    - 이미지, 영상, 음성 등의 비정형 데이터의 분류는 딥러닝이 뛰어난 성능을 보이지만,
    대부분의 정형 데이터 분류 시에는 앙상블이 더 뛰어난 성능을 나타낸다. <br><br>
- **Ensemble 학습의 유형**
    - **Voting**
        
        여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식
        일반적으로 서로 다른 알고리즘을 가진 분류기를 결합
        
        - Hard Voting
            
            예측한 결괏값들 중 다수의 분류기가 결정한 예측값을 최종 결괏값으로 선정
            
        - Soft Voting
            
            분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 
            가장 높은 레이블 값을 최종 결괏값으로 선정
            
            ![6-3-1](https://github.com/eeeeeddy/Machine_Learning/assets/71869717/b715c9a9-faf4-4db4-8588-d69a6ff009c2)
            
    - **Bagging**
        
        여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식
        각각의 분류기가 모두 같은 유형의 알고리즘 기반이며, 데이터 샘플링을 서로 다르게 
        가져가면서 학습을 수행해 Voting을 수행한다.
        대표적인 Bagging 방식으로 **Random Forest** 알고리즘이 있다.
        
        ![6-3-2](https://github.com/eeeeeddy/Machine_Learning/assets/71869717/34ad478f-6323-4112-9743-7c33a42f1798)
        
    - **Boosting**
        
        여러 개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측이 
        틀린 데이터에 대해서는 올바르게 예측할 수 있도록 다음 분류기에게는 가중치를 
        부여하면서 학습과 예측을 진행한다.
        대표적인 Boosting 모듈로 **Gradient Boost, XGBoost LightGBM**이 있다.
        
- **Voting Classifier**
    
    **Ensemble을 이용한 위스콘신 유방암 데이터 세트 예측 분석**
    
    ```python
    import pandas as pd
    
    from sklearn.ensemble import VotingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.datasets import load_breast_cancer
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score
    
    cancer = load_breast_cancer()
    
    data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)
    ```
    
    **Logistic Regression과 KNN을 기반으로 Soft Voting 방식으로 분류기 생성**
    
    ```python
    # 개별 모델은 로지스틱 회귀와 KNN
    lr_clf = LogisticRegression()
    knn_clf = KNeighborsClassifier(n_neighbors=8)
    
    # 개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기 
    vo_clf = VotingClassifier( estimators=[('LR',lr_clf),('KNN',knn_clf)] , voting='soft' )
    
    X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, 
                                                        test_size=0.2 , random_state= 156)
    
    # VotingClassifier 학습/예측/평가. 
    vo_clf.fit(X_train , y_train)
    pred = vo_clf.predict(X_test)
    print('Voting 분류기 정확도: {0:.4f}'.format(accuracy_score(y_test , pred)))
    
    # 개별 모델의 학습/예측/평가.
    classifiers = [lr_clf, knn_clf]
    for classifier in classifiers:
        classifier.fit(X_train , y_train)
        pred = classifier.predict(X_test)
        class_name= classifier.__class__.__name__
        print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test , pred)))
    ```
    
    ```python
    Voting 분류기 정확도: 0.9474
    LogisticRegression 정확도: 0.9386
    KNeighborsClassifier 정확도: 0.9386
    ```
    
    - Bagging과 Boosting은 서로 다른 알고리즘을 기반으로 하고 있지만, 
    대부분 결정 트리 알고리즘 기반
    - 결정 트리 알고리즘은 쉽고 직관적인 분류 기준을 가지고 있지만, 정확한 예측을 위해 
    학습 데이터의 예외 상황에 집착한 나머지 오히려 **과적합**이 발생해 실제 테스트 데이터에서 예측 성능이 떨어지는 현상이 발생하기 쉽다.
    - Ensemble 학습에서는 많은 분류기를 결합해 다양한 상황을 학습하게 함으로써 
    결정 트리 알고리즘의 단점을 극복