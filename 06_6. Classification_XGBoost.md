### XGBoost

- **XGBoost**
  
    - GBM에 기반하는 알고리즘으로 GBM의 단점인 느린 수행 시간 및 과적합 규제 부재 등의 
    문제를 해결한 알고리즘
    - 병렬 CPU 환경에서 병렬 학습이 가능해 기존 GBM보다 빠르게 학습을 완료할 수 있다.
    - 주요 장점
        
        
        | 항목 | 설명 |
        | --- | --- |
        | 뛰어난 예측 성능 | 일반적으로 분류와 회귀 영역에서 뛰어난 예측 성능을 발휘 |
        | GBM 대비 빠른 수행 시간 | 병렬 수행 및 다양한 기능으로 GBM에 비해 빠른 수행 성능을 보장 |
        | 과적합 규제 (Regularization) | 표준 GBM의 경우 과적합 규제 기능이 없으나 XGBoost는 자체에 과적합 규제 기능으로 과적합에 좀 더 강한 내구성을 가질 수 있다. |
        | Tree pruning (나무 가지치기) | max_depth 파라미터로 분할 깊이를 조정하기도 하지만, tree pruning으로 더 이상 긍정 이득이 없는 분할을 가지치기해서 분할 수를 더 줄이는 추가적인 장점이 있다. |
        | 자체 내장된 교차 검증 | 반복 수행 시마다 내부적으로 학습 데이터 세트와 평가 데이터 세트에 대한 교차 검증을 수행해 최적화된 반복 수행 횟수를 가질 수 있다. |
        | 결손값 자체 처리 | 결손값을 자체 처리할 수 있는 기능을 가지고 있다. |
    <br>
- **하이퍼 파라미터**

    - **일반 파라미터**
        - booster
            
            gbtree(tree based model) 또는 gblinear(linear model) 선택, 디폴트는 gbtree
            
        - silent
            
            디폴트는 0이며, 출력 메세지를 나타내고 싶지 않은 경우 1로 설정
            
        - nthread
            
            CPU의 실행 스레드 개수를 조정하며, 디폴트는 CPU의 전체 스레드를 모두 사용한다.
            멀티 코어/스레드 CPU 시스템에서 전체 CPU를 사용하지 않고, 일부 CPU만 사용해 ML 어플리케이션을 구동하는 경우에 변경한다.
            
    - **부스터 파라미터**
        - eta [default=0.3, alias: learning rate]
            
            GBM의 학습률과 같은 파라미터. 0에서 1사이의 값을 지정하며 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률 값
            
        - num_boost_rounds
            
            GBM의 n_estimators와 같은 파라미터
            
        - min_child_weight [default=1]
            
            트리에서 추가적으로 가지를 나눌지를 결정하기 위해 필요한 데이터들의 weight 총합. min_child_weight이 클수록 분할을 자제한다. 과적합을 조절하기 위해 사용
            
        - gamma [default=0, alias: min_split_loss]
            
            트리의 리프 노드를 추가적으로 나눌지를 결정할 최소 손실 감소 값.
            해당 값보다 큰 손실이 감소된 경우에 리프 노드를 분리한다. 값이 클수록 과적합 감소 효과가 있다.
            
        - max_depth [default=6]
            
            트리 기반 알고리즘의 max_depth와 같다. max_depth가 높으면 특정 피처 조건에 특화되어 룰 조건이 만들어지므로 과적합 가능성이 높아지며, 일반적으로 3~10 사이의 값을 적용한다.
            
        - sub_sample [default=1]
            
            GBM의 subsample과 같다. 트리가 커져서 과적합되는 것을 제어하기 위해 데이터를 샘플링하는 비율을 지정한다. 0.5로 지정하면 전체 데이터의 절반을 트리를 생성하는 데 사용한다. 일반적으로 0.5~1 사이의 값을 사용한다.
            
        - colsample_bytree [default=1]
            
            GBM의 max_features와 유사하다. 트리 생성에 필요한 피처를 임의로 샘플링하는 데 사용. 매우 많은 피처가 있는 경우 과적합을 조정하는 데 사용한다.
            
        - lambda [default=1, alias: reg_lambda]
            
            L2 규제 적용 값으로 피처 개수가 많을 경우 적용을 검토하며, 값이 클수록 과적합 감소 효과가 있다.
            
        - alpha [default=0, alias: reg_alpha]
            
            L1 규제 적용 값으로 피처 개수가 많을 경우 적용을 검토하며, 값이 클수록 과적합 감소 효과가 있다.
            
        - scale_pos_weight [default=1]
            
            특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 세트의 균형을 유지하기 위한 파라미터
            
    - **학습 태스크 파라미터**
        - objective
            
            최솟값을 가져야 할 손실 함수를 정의한다. XGBoost는 많은 유형의 손실 함수를 사용할 수 있으며, 분류의 유형(이진/다중)에 따라 달라진다.
            
        - binary:logistic
            
            이진 분류일 때 적용
            
        - multi:softmax
            
            다중 분류일 때 적용하며, 손실 함수가 multi:softmax일 경우에는 레이블 클래스의 개수인 num_class 파라미터를 지정해야 한다.
            
        - multi:softprob
            
            multi:softmax와 유사하나 개별 레이블 클래스의 해당되는 예측 확률을 반환한다.
            
        - eval_metric
            
            검증에 사용되는 함수를 정의한다. 
            
            - rmse : Root Mean Squared Error
            - mae : Mean Absolute Error
            - logloss : Negative log-likelihood
            - error : Binary classification error rate(0.5 threshold)
            - merror : Multiclass classification error rate
            - mlogloss : Multiclass logloss
            - auc : Area under the curve
    <br>
    
    - **과적합 문제에 고려할만한 사항**
      
        - eta 값(0.01~0.1)을 낮춘다. eta 값을 낮출 경우 num_round(또는 n_estimators)는 반대로 높여주어야 한다.
        - max_depth 값을 낮춘다.
        - min_child_weight / gamma 값을 높인다.
        - subsample / colsample_bytree 값을 조정할 경우 트리가 복잡하게 생성되는 것을 막아 과적합 문제에 도움이 될 수 있다.
    
    <br>
    
    **위스콘신 유방암 예측 데이터를 활용한 XGBoost 실습 (Scikit-learn wrapper 사용)**

    ```python
    # 사이킷런 래퍼 XGBoost 클래스인 XGBClassifier 임포트
    from xgboost import XGBClassifier

    xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)
    xgb_wrapper.fit(X_train, y_train)
    w_preds = xgb_wrapper.predict(X_test)
    w_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]
    ```
