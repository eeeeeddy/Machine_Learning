### LightGBM

- **LightGBM**
    - 일반 GBM 계열의 트리 분할 방법과 다르게 리프 중심 트리 분할 방식을 사용한다.
    기존의 대부분 트리 기반 알고리즘은 트리의 깊이를 효과적으로 줄이기 위해 균형 트리 분할 방식을 사용한다.
    - 즉, 최대한 균형 잡힌 트리를 유지하면서 분할하기 때문에 트리의 깊이가 최소화될 수 있다. (오버피팅에 보다 더 강한 구조를 가질 수 있기 때문에 균형 잡힌 트리를 생성한다.)
    - 반대로 균형을 맞추기 위한 시간이 필요하다는 상대적인 단점이 있다. 하지만 LightGBM의 리프 중심 트리 분할 방식은 트리의 균형을 맞추지 않고, 최대 손실 값을 가지는 리프 노드를 지속적으로 분할하면서 트리의 깊이가 깊어지고, 비대칭적인 규칙 트리를 생성한다.
    - 이렇게 최대 손실값을 가지는 리프 노드를 지속적으로 분할해 생성된 규칙 트리는 학습을 반복할수록 결국은 균형 트리 분할 방식보다 예측 오류 손실을 최소화할 수 있다.
    <br><br>
    ![6-7-1](https://github.com/eeeeeddy/Machine_Learning/assets/71869717/606c4293-44d7-4743-9887-bec72d61523e)
    <br><br>
- **XGBoost 대비 LightGBM의 장단점**
    - 더 빠른 학습과 예측 수행 시간
    - 더 작은 메모리 사용량
    - 카테고리형 피처의 자동 변환과 최적 분할
    - 그러나 적은 데이터 세트에 적용할 경우 과적합이 발생하기 쉬움
  <br><br>
- **LightGBM 하이퍼 파라미터**
    - 주요 파라미터
        - num_iterations [default=100]
            
            반복 수행하려는 트리의 개수를 지정한다. 크게 지정할수록 예측 성능이 높아질 수 있으나, 너무 크게 지정하면 과적합으로 성능이 저하될 수 있다.
            
        - learning_rate [default=0.1]
            
            0~1 사이의 값을 지정하며 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률 값
            
        - max_depth [default=-1]
            
            트리 기반 알고리즘의 max_depth와 같다. 0보다 작은 값을 지정하면 깊이의 제한이 없으며, LightGBM은 Leaf wise 기반이므로 깊이가 상대적으로 더 깊다.
            
        - min_data_in_leaf [default=20]
            
            결정 트리의 min_samples_leaf와 같은 값으로 최종 결정 클래스인 리프 노드가 되기 위해서 최소한으로 필요한 레코드 수이며, 과적합을 제어하기 위한 파라미터
            
        - num_leaves [default=31]
            
            하나의 트리가 가질 수 있는 최대 리프 개수
            
        - boosting [default=gbdt]
            
            부스팅의 트리를 생성하는 알고리즘을 기술한다.
            
            - gbdt : Gradient Boosting Decision Tree
            - rf : Random Forest
        - bagging_fraction [default=1.0]
            
            트리가 커져서 과적합되는 것을 제어하기 위해서 데이터를 샘플링하는 비율을 지정
            
        - feature_fraction [default=1.0]
            
            개별 트리를 학습할 때마다 무작위로 선택하는 피처의 비율로 과적합을 막기 위해 사용
            
        - lambda_l2 [default=0.0]
            
            L2 규제 제어를 위한 값으로 피처의 개수가 많을 경우 적용을 검토하며, 값이 클수록 과적합 감소 효과가 있다.
            
        - lambda_l1 [default=0.0]
            
            L1 규제 제어를 위한 값
            
    - Learning Task 파라미터
        - objective
            
            최솟값을 가져야 할 손실 함수를 정의하며, 애플리케이션 유형(회귀, 다중 클래스 분류, 이진 분류)에 따라 손실 함수가 지정된다.
            
- **하이퍼 파라미터 튜닝**
    
    num_leaves의 개수를 중심으로 min_child_samples(min_data_in_leaf), max_depth를 함께 조정하면서 모델의 복잡도를 줄이는 것이 기본 튜닝 방안
    
    - num_leaves는 개별 트리가 가질 수 있는 최대 리프의 개수이며, LightGBM 모델의 복잡도를 제어하는 주요 파라미터. 일반적으로 num_leaves의 개수를 높이면 정확도가 높아지지만, 반대로 트리의 깊이가 깊어지고 모델이 복잡도가 커져 과적합 영향도가 커진다.
    - min_data_in_leaf(min_child_samples)는 과적합을 개선하기 위한 주요 파라미터로
    num_leaves와 학습 데이터 크기에 따라 달라지지만, 보통 큰 값으로 설정하면 트리가 깊어지는 것을 방지한다.
    - max_depth는 명시적으로 깊이의 크기를 제한한다. 위의 파라미터와 결합해 과적합을 개선하는 데 사용한다. <br><br>



    **위스콘신 유방암 예측 데이터를 활용한 LightGBM 실습**

    ```python
    # LightGBM의 파이썬 패키지인 lightgbm에서 LGBMClassifier 임포트
    from lightgbm import LGBMClassifier

    import pandas as pd
    import numpy as np
    from sklearn.datasets import load_breast_cancer
    from sklearn.model_selection import train_test_split

    dataset = load_breast_cancer()
    ftr = dataset.data
    target = dataset.target

    # 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출
    X_train, X_test, y_train, y_test=train_test_split(ftr, target, test_size=0.2, random_state=156 )

    # 앞서 XGBoost와 동일하게 n_estimators는 400 설정. 
    lgbm_wrapper = LGBMClassifier(n_estimators=400)

    # LightGBM도 XGBoost와 동일하게 조기 중단 수행 가능. 
    evals = [(X_test, y_test)]
    lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric="logloss", 
                    eval_set=evals, verbose=True)
    preds = lgbm_wrapper.predict(X_test)
    pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1]
    ```

    ```python
    [1]	valid_0's binary_logloss: 0.565079
    [2]	valid_0's binary_logloss: 0.507451
    [3]	valid_0's binary_logloss: 0.458489
    [4]	valid_0's binary_logloss: 0.417481
    ...
    [142]	valid_0's binary_logloss: 0.196367
    [143]	valid_0's binary_logloss: 0.19869
    [144]	valid_0's binary_logloss: 0.200352
    [145]	valid_0's binary_logloss: 0.19712
    ```

    ```python
    # 예측 성능 평가
    from sklearn.metrics import confusion_matrix, accuracy_score
    from sklearn.metrics import precision_score, recall_score
    from sklearn.metrics import f1_score, roc_auc_score

    def get_clf_eval(y_test, pred=None, pred_proba=None):
        confusion = confusion_matrix( y_test, pred)
        accuracy = accuracy_score(y_test , pred)
        precision = precision_score(y_test , pred)
        recall = recall_score(y_test , pred)
        f1 = f1_score(y_test,pred)
        # ROC-AUC 추가 
        roc_auc = roc_auc_score(y_test, pred_proba)
        print('오차 행렬')
        print(confusion)
        # ROC-AUC print 추가
        print('정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f},\
        F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

    get_clf_eval(y_test, preds, pred_proba)
    ```

    ```python
    오차 행렬
    [[33  4]
    [ 1 76]]
    정확도: 0.9561, 정밀도: 0.9500, 재현율: 0.9870,    F1: 0.9682, AUC:0.9905
    ```