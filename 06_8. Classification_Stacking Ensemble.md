### Stacking Ensemble

- **Stacking Ensemble**
    - Stacking은 개별적인 여러 알고리즘을 서로 결합하여 예측 결과를 도출한다
    (Bagging 및 Boosting과 비슷하다.)
    - 가장 큰 차이점은 개별 알고리즘으로 예측한 데이터를 기반으로 다시 예측을 수행
    - 즉, 개별 알고리즘의 예측 결과 데이터 세트를 최종적인 메타 데이터 세트로 만들어 별도의 머신러닝 알고리즘으로 최종 학습을 수행하고, 테스트 데이터를 기반으로 다시 최종 예측을 수행
    - 두 종류의 모델 필요 (개별적인 기반 모델, 개별 기반 모델의 예측 데이터를 학습 데이터로 만들어 학습하는 최종 메타 모델)
    - 여러개의 모델에 대한 예측값을 합한 후, 즉 Stacking 형태로 쌓은 뒤 이에 대한 예측 수행
        
        ![6-8-1](https://github.com/eeeeeddy/Machine_Learning/assets/71869717/ccd790f5-9097-4b1d-8d57-a15bf910d43e)
        
    
    - Stacking Ensemble 실행 순서
        - M개의 row, N개의 feature(column)을 가진 데이터 세트에 Stacking Ensemble 적용을 가정
        - 학습에 사용할 머신러닝 알고리즘은 총 3개
        
        1. 모델별로 각각 학습을 시킨 뒤 예측을 수행하면 각각 M개의 row를 가진 1개의 레이블 값 도출
        2. 모델별로 도출된 예측 레이블 값을 다시 합하여 (Stacking) 새로운 데이터 세트를 생성
        3. Stacking된 데이터 세트에 대해 최종 모델을 적용해 최종 예측 진행
        <br><br>
        ![6-8-2](https://github.com/eeeeeddy/Machine_Learning/assets/71869717/d8357be5-8e19-462d-9b1d-c4abe8af9f69)
        
    <br><br>
    **위스콘신 유방암 예측 데이터를 활용한 Stacking Ensemble 실습**
    
    - 개별 모델은 KNN, Random Forest, Decision Tree, Adaboost이며, 
    최종 모델은 Logistic Regression 사용
        
        ```python
        import numpy as np
        
        from sklearn.neighbors import KNeighborsClassifier
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.ensemble import AdaBoostClassifier
        from sklearn.tree import DecisionTreeClassifier
        from sklearn.linear_model import LogisticRegression
        
        from sklearn.datasets import load_breast_cancer
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import accuracy_score
        
        cancer_data = load_breast_cancer()
        
        X_data = cancer_data.data
        y_label = cancer_data.target
        
        X_train , X_test , y_train , y_test = train_test_split(X_data , y_label , test_size=0.2 , random_state=0)
        ```
        
        ```python
        # 개별 ML 모델을 위한 Classifier 생성.
        knn_clf  = KNeighborsClassifier(n_neighbors=4)
        rf_clf = RandomForestClassifier(n_estimators=100, random_state=0)
        dt_clf = DecisionTreeClassifier()
        ada_clf = AdaBoostClassifier(n_estimators=100)
        
        # 최종 Stacking 모델을 위한 Classifier생성. 
        lr_final = LogisticRegression(C=10)
        ```
        
        ```python
        # 개별 모델들을 학습. 
        knn_clf.fit(X_train, y_train)
        rf_clf.fit(X_train , y_train)
        dt_clf.fit(X_train , y_train)
        ada_clf.fit(X_train, y_train)
        ```
        
        ```python
        # 학습된 개별 모델들이 각자 반환하는 예측 데이터 셋을 생성하고 개별 모델의 정확도 측정. 
        knn_pred = knn_clf.predict(X_test)
        rf_pred = rf_clf.predict(X_test)
        dt_pred = dt_clf.predict(X_test)
        ada_pred = ada_clf.predict(X_test)
        
        print('KNN 정확도: {0:.4f}'.format(accuracy_score(y_test, knn_pred)))
        print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))
        print('결정 트리 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))
        print('에이다부스트 정확도: {0:.4f}'.format(accuracy_score(y_test, ada_pred)))
        ```
        
        ```python
        KNN 정확도: 0.9211
        랜덤 포레스트 정확도: 0.9649
        결정 트리 정확도: 0.9123
        에이다부스트 정확도: 0.9561
        ```
        
        개별 알고리즘으로부터 예측된 값을 컬럼 레벨로 옆으로 붙여 피처 값으로 만들어, 최종 메타 모델인 Logistic Regression에서 학습 데이터로 사용
        
        ```python
        pred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])
        print(pred.shape)
        
        # transpose를 이용해 행과 열의 위치 교환. 컬럼 레벨로 각 알고리즘의 예측 결과를 피처로 만듦. 
        pred = np.transpose(pred)
        print(pred.shape)
        ```
        
        ```python
        lr_final.fit(pred, y_test)
        final = lr_final.predict(pred)
        
        print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test , final)))
        ```
        
        ```python
        최종 메타 모델의 예측 정확도: 0.9737
        ```
    <br>
- **CV 세트 기반의 Stacking**
    - 과적합을 개선하기 위해 최종 메타 모델을 위한 데이터 세트를 만들 때 교차 검증 기반으로 예측된 결과 데이터 세트를 이용
    - 개별 모델들이 각각 교차 검증으로 메타 모델을 위한 학습용 Stacking 데이터 생성과 예측을 위한 테스트용 Stacking 데이터를 생성
    - 이를 기반으로 메타 모델이 학습과 예측을 수행
        
        Step 1
        각 모델별로 원본 학습/테스트 데이터를 예측한 결과 값을 기반으로 메타 모델을 위한 학습용/테스트용 데이터를 생성
        
        Step 2
        1단계에서 개별 모델들이 생성한 학습용/테스트용 데이터를 모두 Stacking 형태로 합쳐서 메타 모델이 학습할 최종 학습용 데이터 세트 및 최종 테스트 데이터 세트 생성
        
        메타 모델은 최종적으로 생성된 학습 데이터 세트와 원본 학습 데이터의 레이블 데이터를 기반으로 학습한 뒤, 최종적으로 생성된 테스트 데이터 세트를 예측하고, 원본 테스트 데이터의 레이블 데이터를 기반으로 평가
        
    
    1. 3개의 Fold 만큼 반복을 수행하면서 Stacking 데이터를 생성하는 첫 번째 반복
        
        ![6-8-3](https://github.com/eeeeeddy/Machine_Learning/assets/71869717/74588473-9b05-4faa-ba6f-c24c66aa1dab)
        
    2. Stacking 데이터를 생성하는 두 번째 반복
    Fold 내의 학습용 데이터 세트를 변경하고, 첫 번째 그림과 동일한 작업 수행
        
        ![6-8-4](https://github.com/eeeeeddy/Machine_Learning/assets/71869717/55e5cc8b-0b71-4efa-9a51-b63ec554cfc4)
        
    3. Stacking 데이터를 생성하는 세 번째 반복
    세 번째 반복을 수행하면서 Fold 내의 학습용/테스트용 데이터 세트가 변경된다. 
    (세 번 반복을 수행하면서 만들어진 데이터를 합하여 메타 모델에서 사용될 학습/테스트 데이터 생성)
        
        ![6-8-5](https://github.com/eeeeeddy/Machine_Learning/assets/71869717/c5a6f8e4-4aeb-4d32-bd07-d9214ba85c94)
        
    4. 각 모델들이 생성한 학습/테스트 데이터를 모두 합쳐서 최종적으로 메타 모델이 사용할 학습 데이터와 테스트 데이터를 생성
    5. 메타 모델이 사용할 최종 학습 데이터와 원본 데이터의 레이블 데이터를 합쳐서 메타 모델을 학습 한 후 최종 테스트 데이터로 예측을 수행한 뒤, 최종 예측 결과를 원본 테스트 데이터의 레이블 데이터와 비교 및 평가
        
        ![6-8-6](https://github.com/eeeeeddy/Machine_Learning/assets/71869717/da3b306f-7a5e-4ca3-b305-a09c5f7b9ea7)
        
    
    - **CV 세트 기반의 Stacking 실습**
        
        ```python
        from sklearn.model_selection import KFold
        from sklearn.metrics import mean_absolute_error
        
        # 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수. 
        def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ):
            # 지정된 n_folds값으로 KFold 생성.
            kf = KFold(n_splits=n_folds, shuffle=True, random_state=0)
            #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화 
            train_fold_pred = np.zeros((X_train_n.shape[0] ,1 ))
            test_pred = np.zeros((X_test_n.shape[0],n_folds))
            print(model.__class__.__name__ , ' model 시작 ')
            
            for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)):
                #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 
                print('\t 폴드 세트: ',folder_counter,' 시작 ')
                X_tr = X_train_n[train_index] 
                y_tr = y_train_n[train_index] 
                X_te = X_train_n[valid_index]  
                
                #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행.
                model.fit(X_tr , y_tr)       
                #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장.
                train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1)
                #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장. 
                test_pred[:, folder_counter] = model.predict(X_test_n)
                    
            # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성 
            test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)    
            
            #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터
            return train_fold_pred , test_pred_mean
        ```
        
        ```python
        knn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)
        rf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)
        dt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test,  7)    
        ada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)
        ```
        
        ```python
        Stack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)
        Stack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)
        ```
        
        ```python
        lr_final.fit(Stack_final_X_train, y_train)
        stack_final = lr_final.predict(Stack_final_X_test)
        
        print('최종 메타 모델의 예측 정확도: {0:.4f}'.format(accuracy_score(y_test, stack_final)))
        ```
        
        ```python
        최종 메타 모델의 예측 정확도: 0.9737
        ```
        
        마지막으로 Stacking을 이루는 모델은 최적으로 파라미터를 튜닝한 상태에서 Stacking 모델을 만드는 것이 일반적

<br><br>

[참고 사이트](https://velog.io/@dbj2000/ML)