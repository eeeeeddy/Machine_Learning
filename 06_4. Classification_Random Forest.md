### Random Forest

- **Random Forest**
    - Bagging의 대표적인 알고리즘으로 Ensemble 알고리즘 중 비교적 **빠른 수행 속도**를 가지고 있으며, 다양한 영역에서 **높은 예측 성능**을 보인다.
    - 여러 개의 결정 트리 분류기가 전체 데이터에서 Bagging 방식으로 각자의 데이터를 
    샘플링하여 개별적으로 학습을 수행한 뒤 최종적으로 모든 분류기가 Voting을 통해 
    예측 결정 <br><br>
    
    <center><img src="https://github.com/eeeeeddy/Machine_Learning/assets/71869717/9d1b71e1-5f65-4a79-87a1-45c78dac4f1c" width="400" height="450"></center>
    <br>
    
    - 개별적인 분류기의 기반 알고리즘은 Decision Tree이지만 개별 Tree가 학습하는 데이터 세트는 전체 데이터에서 일부가 중첩되게 샘플링된 데이터 세트
    - 이렇게 여러 개의 데이터 세트를 중첩되게 분리하는 것을 **Bootstrapping 분할 방식**이라고 한다.
    <br><br>
    **사용자 행동 인식 데이터 세트를 이용한 Random Forest 실습**
    
        ```python
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.metrics import accuracy_score
        import pandas as pd
        
        X_train, X_test, y_train, y_test = get_human_dataset()
        
        # 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가
        rf_clf = RandomForestClassifier(random_state=0)
        rf_clf.fit(X_train , y_train)
        pred = rf_clf.predict(X_test)
        accuracy = accuracy_score(y_test , pred)
        print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy))
        ```
        
        ```python
        랜덤 포레스트 정확도: 0.9253
        ```
    

- **Random Forest 하이퍼 파라미터 튜닝**
    - 트리 기반의 Ensemble 알고리즘의 단점으로 하이퍼 파라미터가 너무 많아 튜닝을 위한 시간이 많이 소모된다.
    - Random Forest의 하이퍼 파라미터
        - n_estimators (Default = 10)
            
            Random Forest에서 결정 트리의 개수를 지정 
            
        - max_features (Default = sqrt)
            
            최적의 분할을 위해 고려할 최대 피처 개수
            
        - max_depth
            
            트리의 최대 깊이를 규정
            
        - min_samples_leaf
            
            분할이 될 경우 왼쪽과 오른쪽의 브랜치 노드에서 가져야 할 최소한의 샘플 데이터 수
            
        - min_samples_split
            
            노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는 데 사용
            
    
    **GridSearchCV를 이용한 RandomForest의 하이퍼 파라미터 튜닝**
    
    ```python
    from sklearn.model_selection import GridSearchCV
    
    params = {
        'n_estimators':[100],
        'max_depth' : [6, 8, 10, 12], 
        'min_samples_leaf' : [8, 12, 18 ],
        'min_samples_split' : [8, 16, 20]
    }
    
    # RandomForestClassifier 객체 생성 후 GridSearchCV 수행
    rf_clf = RandomForestClassifier(random_state=0, n_jobs=-1)
    grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 )
    grid_cv.fit(X_train , y_train)
    
    print('최적 하이퍼 파라미터:\n', grid_cv.best_params_)
    print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))
    ```
    
    ```python
    최적 하이퍼 파라미터:
    {'max_depth': 10, 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 100}
    최고 예측 정확도: 0.9180
    ```
    
    이때, **n_jobs=-1** 파라미터를 추가하면 모든 CPU 코어를 이용해 학습을 진행한다.
    
    ```python
    rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_leaf=8, min_samples_split=8, random_state=0)
    
    rf_clf1.fit(X_train , y_train)
    pred = rf_clf1.predict(X_test)
    
    print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test , pred)))
    ```
    
    ```python
    예측 정확도: 0.9165
    ```